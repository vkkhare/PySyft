{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Federated Learning Training Plan: Create Plan\n",
    "\n",
    "Let's try to make protobuf-serializable Training Plan and Model that work after deserializing :)\n",
    "\n",
    "Current list of problems:\n",
    " * No support for autograd in Plan tracing (.backward() doesn't work inside the Plan).\n",
    " * `tensor.shape` value seem to be recorded as constant during Plan tracing, so we need to pass `batch_size`, can't take it from tensor itself.\n",
    " * Plan needs a list of all Model params in the argument list, it would be nicer if this list is figured out automatically so you just pass the Model (not sure it's solvable jit might not accept the model as ScriptModule input?)\n",
    " * Plan doesn't return input args from Plan, e.g. if they were updated with inplace operation\n",
    " * others? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/home/varun/PySyft/venv/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.2.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/varun/PySyft/venv/lib/python3.7/site-packages/tf_encrypted/session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Setting up Sandbox...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f75f00b0190>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import syft as sy\n",
    "import torch as th\n",
    "from torch import jit\n",
    "from syft.serde import protobuf\n",
    "import os\n",
    "from syft.execution.state import State\n",
    "from syft.execution.placeholder import PlaceHolder\n",
    "\n",
    "\n",
    "sy.hook(globals())\n",
    "# force protobuf serialization for tensors\n",
    "hook.local_worker.framework = None\n",
    "th.random.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This utility function will serialize any object to protobuf binary and save to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def serializeToBinPb(worker, obj, filename):\n",
    "    pb = protobuf.serde._bufferize(worker, obj)\n",
    "    bin = pb.SerializeToString()\n",
    "    print(\"Writing %s to %s/%s\" % (obj.__class__.__name__, os.getcwd(), filename))\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 1: Define the model\n",
    "\n",
    "This model will train on MNIST data, it's very simple yet can demonstrate learning process.\n",
    "There're 2 linear layers: \n",
    "\n",
    "* Linear 784x392\n",
    "* ReLU\n",
    "* Linear 392x10 \n",
    "\n",
    "Not using nn.Module or nn.Linear for now, just vanilla class and tensors.\n",
    "No autograd, gradients are hand-coded. \n",
    "\n",
    "As no loops supported inside Plan, \n",
    "we can't iterate over parameters so everything that works with params\n",
    "(get/set, step) is moved into the model to make Plan code more generic.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net():\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.W1 = th.randn(392, 784) / th.sqrt(th.tensor(784.))\n",
    "        self.b1 = th.zeros(392)\n",
    "        self.W2 = th.randn(10, 392) / th.sqrt(th.tensor(392.))\n",
    "        self.b2 = th.zeros(10)\n",
    "        self.update_fn = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.Z1 = X @ self.W1.t() + self.b1\n",
    "        self.A1 = th.nn.functional.relu(self.Z1)\n",
    "        self.Z2 = self.A1 @ self.W2.t() + self.b2\n",
    "        return self.Z2\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.W1, self.b1, self.W2, self.b2\n",
    "\n",
    "    def set_params(self, *model_params):\n",
    "        self.W1, self.b1, self.W2, self.b2 = model_params\n",
    "\n",
    "    def grad(self, X, error):\n",
    "        Z1_grad = (error @ self.W2) * (self.Z1 > 0).float()\n",
    "        W1_grad = Z1_grad.t() @ X\n",
    "        b1_grad = Z1_grad.sum(0)\n",
    "        W2_grad = error.t() @ self.A1 \n",
    "        b2_grad = error.sum(0)\n",
    "        return W1_grad, b1_grad, W2_grad, b2_grad\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 2: Define Training Plan\n",
    "### Loss function \n",
    "Batch size needs to be passed because otherwise `target.shape[0]` will be saved as `1` constant during Plan trace with dummy data.\n",
    "Grad is also returned here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_with_logits(output, target, batch_size):\n",
    "    probs = th.nn.functional.softmax(output, dim=1)\n",
    "    loss = -(target * th.log(probs)).mean().reshape(1)\n",
    "    loss_grad = (probs - target) / (batch_size * target.shape[1])\n",
    "    return probs, loss, loss_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Optimization function\n",
    " \n",
    "Just updates weights with grad*lr.\n",
    "\n",
    "Note: can't do inplace update and return this value from the Plan, which is potentially bad for memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def naive_sgd(param, grad, **kwargs):\n",
    "    return param - grad * kwargs['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training Plan procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_params = model.get_params()\n",
    "\n",
    "# define plan input dimensions\n",
    "X_size = (-1, 784)\n",
    "y_size = (-1, 10)\n",
    "scalar_size = (1,)\n",
    "model_params_shapes = [p.shape for p in model_params]\n",
    "\n",
    "args_shape = [\n",
    "    X_size,  # X\n",
    "    y_size,  # y\n",
    "    scalar_size,  # batch_size\n",
    "    scalar_size,  # lr\n",
    "    *model_params_shapes  # *model_params\n",
    "]\n",
    "\n",
    "@sy.func2plan(args_shape=args_shape)\n",
    "def training_plan(X, y, batch_size, lr, *model_params):\n",
    "    # inject params into model\n",
    "    model.set_params(*model_params)\n",
    "\n",
    "    # forward pass\n",
    "    output = model.forward(X)\n",
    "    \n",
    "    # loss\n",
    "    probs, loss, loss_grad = cross_entropy_with_logits(output, y, batch_size)\n",
    "\n",
    "    # backprop\n",
    "    grads = model.grad(X, loss_grad)\n",
    "\n",
    "    # step\n",
    "    updated_params = [naive_sgd(param, grads[i], lr=lr) \n",
    "                      for i, param in enumerate(model_params)]\n",
    "    \n",
    "    # accuracy\n",
    "    pred = th.argmax(probs, dim=1)\n",
    "    target = th.argmax(y, dim=1)\n",
    "    acc = pred.eq(target).float().sum() / batch_size\n",
    "\n",
    "    return (\n",
    "        loss,\n",
    "        acc,\n",
    "        *updated_params\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's look inside the Plan and print out the list of operations recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Plan' object has no attribute 'find_placeholders'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-dfe60f4a8ebe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Inputs:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_plan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_placeholders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtag_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margToStr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Plan' object has no attribute 'find_placeholders'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "input_names = [\n",
    "    \"X\", \"y\", \"batch_size\", \"lr\",\n",
    "    \"W1\", \"b1\", \"W2\", \"b2\"\n",
    "]\n",
    "\n",
    "output_names = [\n",
    "    \"loss\", \"acc\",\n",
    "    \"upd_W1\", \"upd_b1\", \"upd_W2\", \"upd_b2\"\n",
    "]\n",
    "\n",
    "def placeholderToStr(ph: PlaceHolder):\n",
    "    ret = ''\n",
    "    for tag in ph.tags:\n",
    "        if re.search('input', tag):\n",
    "            idx = int(tag.split(\"-\")[1])\n",
    "            return input_names[idx]\n",
    "        elif re.search('output', tag):\n",
    "            idx = int(tag.split(\"-\")[1])\n",
    "            return output_names[idx]\n",
    "        else:\n",
    "            ret = \"var_\" + tag.split(\"#\")[1]\n",
    "    return ret\n",
    "            \n",
    "def argToStr(arg):\n",
    "    if isinstance(arg, tuple):\n",
    "        return \", \".join(list(map(argToStr, arg)))\n",
    "    if isinstance(arg, PlaceHolder):\n",
    "        return placeholderToStr(arg)\n",
    "    else:\n",
    "        return str(arg)\n",
    "\n",
    "def tag_sort(keyword):\n",
    "    def extract_key(placeholder):\n",
    "        for tag in placeholder.tags:\n",
    "            if keyword in tag:\n",
    "                return int(tag.split(\"-\")[-1])\n",
    "    return extract_key\n",
    "\n",
    "print(\"Inputs:\")\n",
    "for inp in sorted(training_plan.find_placeholders(\"input\"), key=tag_sort(\"input\")):\n",
    "    print(argToStr(inp))\n",
    "\n",
    "print(\"\\nActions:\")\n",
    "for action in training_plan.actions:\n",
    "    expr = [argToStr(action.return_ids), ' = ']\n",
    "    if action.target is None:\n",
    "        expr += [action.name, '(']\n",
    "    else:\n",
    "        expr += [argToStr(action.target), '.', action.name, '(']\n",
    "\n",
    "    if len(action.args):\n",
    "        expr += argToStr(action.args)\n",
    "\n",
    "    if action.kwargs:\n",
    "        expr += ', ', str(action.kwargs)\n",
    "    \n",
    "    expr += [')']\n",
    "    print(\"\".join(expr))\n",
    "\n",
    "print(\"\\nOutputs:\")\n",
    "for out in sorted(training_plan.find_placeholders(\"output\"), key=tag_sort(\"output\")):\n",
    "    print(argToStr(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 3: JIT Trace Training Plan\n",
    "\n",
    "Note: Plan expects everything to be a tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def __call__(argument_0: Tensor,\n",
      "    argument_1: Tensor,\n",
      "    argument_2: Tensor,\n",
      "    argument_3: Tensor,\n",
      "    argument_4: Tensor,\n",
      "    argument_5: Tensor,\n",
      "    argument_6: Tensor,\n",
      "    argument_7: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
      "  _0 = torch.matmul(argument_0, torch.t(argument_4))\n",
      "  _1 = torch.add(_0, argument_5, alpha=1)\n",
      "  _2 = torch.relu(_1)\n",
      "  _3 = torch.add(torch.matmul(_2, torch.t(argument_6)), argument_7, alpha=1)\n",
      "  _4 = torch.softmax(_3, 1, None)\n",
      "  _5 = torch.mean(torch.mul(argument_1, torch.log(_4)), dtype=None)\n",
      "  _6 = torch.neg(torch.reshape(_5, [1]))\n",
      "  _7 = torch.div(torch.sub(_4, argument_1, alpha=1), torch.mul(argument_2, CONSTANTS.c0))\n",
      "  _8 = torch.matmul(_7, argument_6)\n",
      "  _9 = torch.to(torch.gt(_1, 0), 6, False, False, None)\n",
      "  _10 = torch.mul(_8, _9)\n",
      "  _11 = torch.matmul(torch.t(_10), argument_0)\n",
      "  _12 = torch.sum(_10, [0], False, dtype=None)\n",
      "  _13 = torch.matmul(torch.t(_7), _2)\n",
      "  _14 = torch.sum(_7, [0], False, dtype=None)\n",
      "  _15 = torch.sub(argument_4, torch.mul(_11, argument_3), alpha=1)\n",
      "  _16 = torch.sub(argument_5, torch.mul(_12, argument_3), alpha=1)\n",
      "  _17 = torch.sub(argument_6, torch.mul(_13, argument_3), alpha=1)\n",
      "  _18 = torch.sub(argument_7, torch.mul(_14, argument_3), alpha=1)\n",
      "  _19 = torch.eq(torch.argmax(_4, 1, False), torch.argmax(argument_1, 1, False))\n",
      "  _20 = torch.sum(torch.to(_19, 6, False, False, None), dtype=None)\n",
      "  _21 = (_6, torch.div(_20, argument_2), _15, _16, _17, _18)\n",
      "  return _21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_trace = th.randn(1, 784)\n",
    "y_trace = th.randn(1, 10)\n",
    "lr = th.tensor(0.001)\n",
    "batch_size = th.tensor(1)\n",
    "\n",
    "# remove actual function, so plan executes recorded ops\n",
    "training_plan.forward = None\n",
    "\n",
    "# jit trace\n",
    "training_plan_torchscript = th.jit.trace(training_plan.__call__, (X_trace, y_trace, batch_size, lr, *model_params))\n",
    "\n",
    "# Let's see\n",
    "print(training_plan_torchscript.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 4: Serialize!\n",
    "\n",
    "Now it's time to serialize model params and plans to protobuf and save them for further usage:\n",
    " * In \"Execute Plan\" notebook, we load and execute these plans & model, from Python.\n",
    " * In \"Host Plan\" notebook, we send these plans & model to PyGrid, so it can be executed from other worker (e.g. syft.js).\n",
    "\n",
    "**NOTE:**\n",
    " * We don't serialize full Model, only weights. How the Model is serialized is TBD.\n",
    "   State is suitable protobuf class to wrap list of Model params tensors.\n",
    " * Plan containing list of operations is serialized to syft_proto.execution.v1.Plan \n",
    " while torchscript Plan is serialized to syft_proto.types.torch.v1.ScriptFunction. \n",
    " In the future they will converge to syft_proto.execution.v1.Plan, see https://github.com/OpenMined/PySyft/issues/2994#issuecomment-595333791"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Plan to /home/varun/PySyft/examples/experimental/FL Training Plan/tp_ops.pb\n",
      "Writing ScriptFunction to /home/varun/PySyft/examples/experimental/FL Training Plan/tp_ts.pb\n",
      "Writing State to /home/varun/PySyft/examples/experimental/FL Training Plan/model_params.pb\n"
     ]
    }
   ],
   "source": [
    "serializeToBinPb(hook.local_worker, training_plan, \"tp_ops.pb\")\n",
    "serializeToBinPb(hook.local_worker, training_plan_torchscript, \"tp_ts.pb\")\n",
    "\n",
    "# wrap weights in State to serialize\n",
    "model_params_state = State(\n",
    "    owner=hook.local_worker,\n",
    "    state_placeholders=[PlaceHolder().instantiate(param) for param in model_params]\n",
    ")\n",
    "\n",
    "serializeToBinPb(hook.local_worker, model_params_state, \"model_params.pb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mnist_train.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.values[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"label.csv\", labels.astype(np.int32), delimiter=\",\")\n",
    "np.savetxt(\"train.csv\", data.astype(np.int32), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
